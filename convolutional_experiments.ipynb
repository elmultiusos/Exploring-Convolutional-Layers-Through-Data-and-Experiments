{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Convolutional Layers: Fashion-MNIST Experiments\n",
    "\n",
    "## 1. Context and Objective\n",
    "\n",
    "In this notebook, we will explore the impact of Convolutional Neural Networks (CNNs) on image classification tasks. We will move beyond treating neural networks as black boxes and analyze how architectural choices affect performance.\n",
    "\n",
    "**Dataset Selected**: Fashion-MNIST\n",
    "- **Description**: A dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images.\n",
    "- **Justification**: It serves as a more challenging direct replacement for the original MNIST dataset. While MNIST digits are easy to classify with simple dense networks, Fashion-MNIST requires capturing more complex spatial patterns (textures, shapes of clothing), making it ideal for demonstrating the advantages of inductive bias in CNNs.\n",
    "\n",
    "**Tasks covers in this notebook**:\n",
    "1. Dataset Exploration (EDA)\n",
    "2. Baseline Model (Dense Only)\n",
    "3. CNN Architecture Design\n",
    "4. Controlled Experiments (Kernel Size Analysis)\n",
    "5. Interpretation\n",
    "6. SageMaker Deployment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration (EDA)\n",
    "We start by loading the dataset and understanding its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Let's visualize random samples from each class to understand the complexity of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    # Find the first image of each class\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_train[idx], cmap='gray')\n",
    "    plt.title(class_names[i])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution\n",
    "Checking if the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.bar(class_names, counts)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Class Distribution in Training Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Neural networks converge faster on normalized data. We also need to reshape the images to include the channel dimension (H, W, C), which is (28, 28, 1) for grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train_norm = x_train.astype('float32') / 255.0\n",
    "x_test_norm = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN (batch, height, width, channels)\n",
    "x_train_cnn = x_train_norm.reshape((-1, 28, 28, 1))\n",
    "x_test_cnn = x_test_norm.reshape((-1, 28, 28, 1))\n",
    "\n",
    "print(f\"New shape for CNN: {x_train_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model (Non-Convolutional)\n",
    "We implement a simple Multi-Layer Perceptron (MLP) containing only Dense (Fully Connected) layers. This ignores the 2D spatial structure of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "baseline_model = build_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline\n",
    "history_baseline = baseline_model.fit(x_train_cnn, y_train, epochs=10, validation_data=(x_test_cnn, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CNN Architecture Design\n",
    "We design a custom CNN. \n",
    "**Architecture Decisions:**\n",
    "- **Conv2D Layers**: To extract spatial features.\n",
    "- **ReLU Activation**: To introduce non-linearity.\n",
    "- **MaxPooling**: To provide spatial invariance and reduce dimensionality.\n",
    "- **Structure**: Conv -> Pool -> Conv -> Pool -> Flatten -> Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(kernel_size=(3,3)):\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, kernel_size, activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, kernel_size, activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Classification Head\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Controlled Experiments: Kernel Size\n",
    "We investigate the effect of Kernel Size on the model's performance.\n",
    "- **Model A**: Kernel Size 3x3 (Standard, captures fine details)\n",
    "- **Model B**: Kernel Size 5x5 (Larger receptive field, might miss fine details or over-smooth)\n",
    "\n",
    "We keep everything else (filters, layers, pooling) constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment A: 3x3 Kernels\n",
    "print(\"Training CNN with 3x3 Kernels...\")\n",
    "cnn_3x3 = build_cnn_model(kernel_size=(3,3))\n",
    "cnn_3x3.summary()\n",
    "history_3x3 = cnn_3x3.fit(x_train_cnn, y_train, epochs=10, validation_data=(x_test_cnn, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment B: 5x5 Kernels\n",
    "print(\"Training CNN with 5x5 Kernels...\")\n",
    "cnn_5x5 = build_cnn_model(kernel_size=(5,5))\n",
    "history_5x5 = cnn_5x5.fit(x_train_cnn, y_train, epochs=10, validation_data=(x_test_cnn, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_baseline.history['val_accuracy'], label='Baseline (Dense)')\n",
    "plt.plot(history_3x3.history['val_accuracy'], label='CNN (3x3)')\n",
    "plt.plot(history_5x5.history['val_accuracy'], label='CNN (5x5)')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_baseline.history['val_loss'], label='Baseline (Dense)')\n",
    "plt.plot(history_3x3.history['val_loss'], label='CNN (3x3)')\n",
    "plt.plot(history_5x5.history['val_loss'], label='CNN (5x5)')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretation and Architectural Reasoning\n",
    "\n",
    "### Why did the CNN outperform the Baseline?\n",
    "The CNN outperforms the dense baseline because of **Inductive Bias** tailored for images. Dense layers treat every pixel as independent and fully connected to every neuron in the next layer, ignoring the spatial relationship between pixels. CNNs assume that:\n",
    "1.  **Local Connectivity**: Pixels close to each other form meaningful features (edges, textures).\n",
    "2.  **Translation Invariance**: A feature (like a button or a corner) is the same regardless of where it appears in the image, thanks to weight sharing in convolution filters.\n",
    "\n",
    "### Effect of Kernel Size\n",
    "Comparing 3x3 vs 5x5 kernels helps us understand receptive fields. \n",
    "- **3x3** kernels are generally preferred in modern architectures (like VGG) because stacking them increases non-linearity while keeping parameter count low.\n",
    "- **5x5** kernels capture a larger area in one go but might be efficient for simple patterns (like in LeNet-5). In Fashion-MNIST, where objects are centered and structures are relatively simple, both perform well, but 3x3 often yields slightly better generalization or parameter efficiency.\n",
    "\n",
    "### When NOT to use Convolution?\n",
    "Convolution is not appropriate when data **does not have a grid-like topology** or local spatial correlation. For example:\n",
    "- Tabular data (rows/columns in a spreadsheet).\n",
    "- Sets of independent measurements.\n",
    "- Graphs (use Graph Neural Networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment in SageMaker\n",
    "\n",
    "To deploy this model in AWS SageMaker, we need to wrap the training code into a script `train.py` and use the SageMaker Python SDK.\n",
    "\n",
    "**Note**: This section requires an active AWS session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def train(args):\n",
    "    # Load Data\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    x_train = x_train.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
    "    x_test = x_test.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
    "    \n",
    "    # Define Model (CNN 3x3)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    print(\"Starting training...\")\n",
    "    model.fit(x_train, y_train, epochs=args.epochs, batch_size=args.batch_size)\n",
    "    \n",
    "    # Save Model (SageMaker expects model in /opt/ml/model)\n",
    "    model_dir = os.environ.get('SM_MODEL_DIR', '/opt/ml/model')\n",
    "    model.save(f\"{model_dir}/00000001\")\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    args = parser.parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# Define Sagemaker Session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() # Needs AWS Configuration\n",
    "\n",
    "# Create Estimator\n",
    "estimator = TensorFlow(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    framework_version='2.11',\n",
    "    py_version='py39',\n",
    "    hyperparameters={'epochs': 10}\n",
    ")\n",
    "\n",
    "# Note: Uncomment to run deployment\n",
    "# estimator.fit()\n",
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
